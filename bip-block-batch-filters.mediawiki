<pre>
  BIP: Block Batch Filters for Light Clients
  Layer: Peer Services
  Title: Block Batch Filters for Light Clients
  Author: Aleksey Karpov <admin@bitaps.com>
  Comments-Summary: None yet
  Comments-URI: 
  Status: Draft
  Type: Standards Track
  Created: 2019-09-20
  License: PD
</pre>

== Abstract ==

This BIP describes a new filter types on blocks data, for use in the BIP 157 light client protocol. 
The filter construction proposed is an alternative to BIP 158 filters, with lower filters size, lower false positive rate and  separated by address types.

== Motivation ==

Block filters designed to help light clients analize block contents without downloading full blocks and minimize the expected bandwidth consumed. The main difference from the BIP 37 approach is no privacy leakage and no Denial of Service (DoS) attack vector for nodes. BIP 158 defines initial filter type <code>basic</code>. This BIP proposed more effective and flexible alternative.

First of all let's determine reasonable number of addresses tracked by a light client in most common case. We will use this value for false positive rate tests in this BIP.

To achieve privacy and security, wallets must follow the "no address reuse" rule. This means that each payment
received or sent uses one or more new addresses. Suppose wallet makes 2-3 payments daily, during the year the number 
of used addresses increases to about 1000 pieces. All addresses that were used at least once should be monitored
for new payments. Assume in this BIP that a reasonable estimated number of wallet monitoring addresses is 10 000 pieces. 

The main block filter parameters that have been improved and redesigned in this BIP:


==== Separating filters by address type ====

Separating filters by address type, allows light client not to download redundant information without compromising privacy. It will significantly reduce global bandwidth usage, because the most of wallets are processing only one type of addresses.


==== Transaction filters ====

Using transaction filters makes it possible to use the advantages of filtering for unconfirmed transactions, as well as requesting only those transactions that are necessary for the light client when we have positive test in block filter.
More details decribed here https://github.com/bitaps-com/bips/blob/master/bip-mempool-transactions-filters.mediawiki.


==== Commitment ====

This is one of the useful properties that filters should meet, filter construction should use deterministic algorithm to give ability publish commitment for filters like witness commitment is in an OP_RETURN output of the coinbase transaction. Adding the task of creating filters to the rules of consensus, forcing miners to create filters and make filters commitment, unlocks the protection of spv-nodes against payment-hiding attacks. At the current moment, lightweight clients  trust the nodes and rely on the fact that most nodes are honest.

Adding block filters commitment to coinbase transaction, as the block validation rule will improve the lightweight client protocol by moving from trust to validation on consensus layer. Filters described here are fully adapted for this improvement but the filters commitment is beyond the scope of this BIP and may be added in a future soft fork.


==== Total filters size and false positive rate ====

The frequency of false positives should not be high and provide effective filtering to reduce bandwidth consumption, but to achieve this we must increase the information entropy of the elements, which leads to an increase in size. The most effective algorithm for data compression and the balance between the size of the filters and the number of false positives are the main tasks to be solved in this BIP.

Total filters size calcalted for range  0 -> 596,735 blocks.

* <b>BIP 158</b> total size: 4.3 Gb
* <b>Batch 1008 blocks</b> total size: 3.36 Gb

Batch filters size about 22% smaller than the BIP 158.


==== Batching ====

About 94% of all addresses recorded in bitcoin blockchain have zero balance at this moment, this means that at 
least 2 records for 94% of addresses in blockchain. Each spent utxo have 1 creating and 1 spending records. The presence of duplicate elements is also due to the fact that many addresses are used several times.

Combining filters into batches of blocks makes it possible to reduce the size of filters. Dividing the filter into 2 parts allows encoding data more compactly. The first part is the unique addresses for the block in the batch. The second part is duplicate elements in the current block, which have already met in the first part of filters in the batch. But duplicates for unique elements of the current block are not recorded, since the entry test will be successfully passed in the first part of the block filter. 

On the picture below shows the structure of the batch. Unique addresses for block filters highlighted in blue. All unique elements are indexed. Duplicates are written as pointers instead of the values themselves. 

<img src=bip-block-batch-filters/batchf.png></img>
On picture <code>Block 2</code> have address <code>E</code> as duplicate element. <code>Block 570</code> have address <code>G</code> as duplicate element written as pointer 6. Separation of unique elements and duplicates also gives some improvement in the filter scanning speed, because checking the second part of the filter is only necessary if we have affected addresses in recent batch and we need check only small set of affected addresses.
Returning to the example above, if we are tracking a set of addresses <code>{A, J, Q}</code> only starting from <code>Block 570</code> (inclusive), we need to check the part with duplicates. Starting with <code>Block 570</code>, we need to check only the element <code>J</code>, and then from <code>Block 1567</code> we need to check set <code>{J, Q}</code>, in other words, it’s enough for us to check for duplicates only for our affected elements in the last batch.

Using a batch approach with pointers to unique elements, described in detail below, we get a good degree of data compression. To encode 1 unique element (4 bytes in size) we need only 2.6 bytes and about 1.4 bytes to encode 1 duplicated element with a counter.


==== Light client filters test perfomance ====

BIP 158 filters use an array with the size of the elements <code>N * M</code> to map all possible block elements hashes. Where <code>N</code> count of elements in block and <code>M = 784931</code> constant represent <code>1/M</code> probabilty rate. In this case we are dealing with floating parameter <code>F = N * M</code> for Golomb coding, also BIP 158 use randomization vector for siphash function for each block. In other words, to check the block filter, we need to recalculate for each block a new array of addresses that are monitored by light wallet in accordance with the randomization vectors and the parameter <code>F</code>.
In case we want rescan all blocks and check 10 000 addresses we have to calculate about 6 000 000 000 hash + map_into_range functions.

In this BIP purposed shema with constat parameter <code>F = 2 ^ 32</code> and dinamicaly calculated parameter <code>P</code> for Golomb coding. No randomization vectors for each block and constant <code>F</code> remove computation overhead with almost same false positive rate (false positive test slightly better than BIP 158). Also an additional speed improvement is related to fact that the design of divided filters into 2 parts allows us to check part 2 only for set of addresses that have a positive test in the first part filters within current batch. This reduces the number of elements to be checked by almost half.


== Specification ==

==== Filter types ====

* <code>P2PK - 0x01</code>
* <code>P2PKH  - 0x02</code>
* <code>P2SH  - 0x04</code>
* <code>P2WPKH  - 0x08</code>
* <code>P2WPSH  - 0x16</code>

Filter types support filter combinations, that means in case we need block filters for all address types we should calculate
0x01 + 0x02 + 0x04 + 0x08 + 0x16 = 0x25. To reduce bandwidth usage, the light client should request only the necessary filter types. In case combined filter types are requested, the filters are serialized sequentially according to the type index.

==== Transaction filter ====

For each transaction filters are derived containing sets of uique addresses associated with transaction. Set of unique elements whose order does not matter coded with Golomb-Rice code. Details of Golomb-Rice coding the same as described below in Block filter. More details here https://github.com/bitaps-com/bips/blob/master/bip-mempool-transactions-filters.mediawiki.


==== Block filter ====
For each block, filters are derived containing sets of addresses associated with the block transaction filters. Filter is probabilistic structure which matches all items in the set with probability 1, and matches other items with probability 1/M for some integer parameter M. In BIP 158 all block addresses map into set of distinct integers within the range <code>[0, F)</code>, where <code>F = N * M<code>, <code>N</code> is count of block elements and M is BIP 158 constant <code>M = 784931</code>.

In this BIP used constant parameter <code>F= 2 ^ 32</code>. False positve test probability is <code>N / F</code>. For compression, the Golomb-Rice code with the parameter P is used, where P is determined for each filter based on the values in the set. This approach allows us to preserve the simplicity and speed of sets comparison process, exclude computation overhead for each set.

All block addresses are divided by type of address and a separate filter is formed for each type of address. The filter consists of unique elements, duplicates in the form of pointers and a fingerprint of transaction filters. A transaction filter fingerprint is a hash from serialized block transaction filters. We need this fingerprint for the following purposes.
In case we have positive test in block filter we can get transaction information in 2 ways. The first method is to request a complete block from the network, the second more effective is to request filters for all transactions of the block and determine the exact transaction that we must request from the network. 

This BIP is fully adopted for block filters commitment and transaction filters fingerprint give for light client ability to verify transaction filters with commitment.

The following example shows the need to use the fingerprint for transaction filters. Light client downloaded block filter and got positive test for monitoring address. Positive test means that one or more blcok transactions affected. Block filter could be verified via commitment (we assume that a filters commitment is a block validation rule), but when ligth client request block transaction filters we cant verify that received filters is correct and do not try to hide payments. Download transaction filters is about 40-60 kbyte of data instead of 1-1.2 MB complete block. 

Light client can use block merkle root to determine that all transaction filters are received. But we can't verify content of this filters. Malicious node could craft transaction filters and show only 1 transaction instead of 2 or more. Because we have no counters of elements in block filters we can't determain this attack. Transaction filters fingerprint solve this problem by serializing transaction filters using a deterministic algorithm and obtain hash fingerprint from this data. In case fingerprint not match light client should request complete block and ban malicious node from connection pool.

Block filter created as follows: 

# serialize all block transaction filters and get fingerprint
# create address set with block unique addresses within recent batch
# create duplicates set with pointers for all duplicated addresses within recent batch
# encode address set with Golomb coding
# encode duplicates set with Golomb coding
# serialize filter and get commitment hash

It is important to note that transaction filters consist only of unique elements, where uniqueness is determined within the transaction.


==== Filter elements ====

The filters elements derived from transaction inputs and outputs hashes for standard script types (pubkey hashes, script hashes). Coinbase inputs are not used. This information is minimum sufficient for light wallet to sync blockchain. Script/Pubkey hashes is choosen instead of raw script because output script can’t be recovered for P2PK addresses in case no information about public key. This means that watch only wallet, with imported P2PK addresses from base58 format, not able to reconstruct output script without downloading transactions where this script was seen first time and will not be able to check filters.


==== Filter structure ====

{|
! Field Name !! Type !! Size  || Purpose
|-
| elements_set_bytes || CompactSize || 1 - 5 bytes  || A variable length integer representing the size of the Golomb Coded Set with uniqie elements
|-
| elements || Binary blob || variable || GCS with unique  elements
|-
| duplicates_set_bytes || CompactSize || 1 - 5 bytes || A variable length integer representing the size of the GCS with  duplicate elements
|-
| duplicates || Binary blob || variable || GCS with  duplicates 
|-
| fingerprint ||  Binary blob  || 20 bytes || block transaction filters fingerprint
|-
|}





==== Commitment structure ====
TODO


== Light client filter processing schema ==

TODO

== Compatibility ==

Older clients remain fully compatible and interoperable after this change. 

Clients which do not implement this BIP remain fully compatible after this change using existing protocols, because this is 
additional node service and changes not affected to exist protocol.

== Reference Implementation ==
TODO

== References ==

<references/>

== Copyright ==

This document is placed in the public domain.
