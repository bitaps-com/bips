<pre>
  BIP: Block Batch Filters for Light Clients
  Layer: Peer Services
  Title: Block Batch Filters for Light Clients
  Author: Aleksey Karpov <admin@bitaps.com>
  Comments-Summary: None yet
  Comments-URI: 
  Status: Draft
  Type: Standards Track
  Created: 2019-09-20
  License: PD
</pre>

== Abstract ==

This BIP describes a new filter types on blocks data, for use in the BIP 157 light client protocol. 
The filter construction proposed is an alternative to BIP 158 filters, with lower filters size, lower false positive rate and  separated by address types.

== Motivation ==

Block filters designed to help light clients analize block contents without downloading full blocks and minimize the expected bandwidth consumed. The main advantages of the BIP 37 approach are the lack of privacy leakage and the absence of a denial of service (DoS) attack vector for nodes. BIP 158 defines initial filter type <code>basic</code>. This BIP proposed more effective and flexible alternative.

Fisrt of all lets determine reasanable number of addresses tracked by a light client in most common case.

To achieve privacy and security, wallets must follow the "no address reuse" rule. This means that each payment
received or sent uses one or more new addresses. Suppose wallet makes 2-3 payments per day, during the year the number 
of used addresses increases to about 1000 pieces. All addresses that were used at least once should be monitored
for new payments. Assume in this BIP that a reasonable estimated number of wallet monitoring addresses is 10 000 pieces. 

Сonsider the key parameters that the filters should match:


==== False positive rate ====

False positive rate should not be high and provide effective filtering to reduce bandwidth consumption. False positive rate in BIP 158 is not low enough. Lower bandwidth consumption can be achieved with a lower false positive filter and a larger fiters size with the same scheme. Below provided result of test with  10,000 addresses in monitoring for 2,000 blocks (7,300 elements inside each block):

<pre>
Parameters for BIP 158 Golomb coded set: M=784931 P=19
False positive tests = 32 from 2,000 blocks 
Filters size:  36.64 Mb
Downloaded 68.64 Mb (32 blocks included);
</pre>

Size of downloaded false positive blocks (~32 Mb) can be excluded by use filters with lower false positive rate . 

<pre>
Parameters for BIP 158 Golomb coded set: M=54975581 P=25
False positive blocks 0 from 2,000 blocks 
Download 57.64 Mb  
Filters size:  57.64 Mb
</pre>

Based on the results of the experiment, we can see that bandwidth consumption reduced about 16% when we use lower false positive rate despite the increased filter size. 


==== Separating filters by address type ====

Separating filters by address type, allows light client not to download redundant information without compromising privacy. It will significantly reduce global bandwidth usage, because the most of wallets are processing only one type of addresses.


==== Batching ====

About 94% of all addresses recorded in bitcoin blockchain have zero balance at this moment, this means that at 
least 2 records for 94% of addresses in blockchain. Each spent utxo have 1 creating and 1 spending records. The presence of duplicate elements is also due to the fact that many addresses are used several times.

Combining filters for ranges of blocks to batches gives another opportunity to reduce the size of filters. Separation of filters of a batch of filter elements into 2 parts allows encoding data more compactly.First part is unique addresses within batch and second part is duplicate element in recent batch.

On picture below you can see first part of unique addresses for block filters highlighted in blue. Duplicated elements below, have a distinctive feature. They form a separate set for encoding which a sufficiently high FPS is enough, that means save alot of space.

<img src=bip-block-batch-filters/batching_b.png></img>

For example, an SPV wallet keeps track set of addresses {Q,V,C,E}. On block 1 we have positive test and download this block. On block 2 we have no positive test on first part and then we check second part. The second part consists only of duplicates from the beginning of the block till recent block. Our task is get check is this duplicated elements belong to our addresses {Q,V,C,E}. To solve this task we should compare set of  duplicates from recent block with a set of addresses we track. But we can reduce our monitoring set to a smaller set, consisting of the tracked addresses that were used from the beginning of the range till recent block. For block 2 on example compare {E} and {B}, for block 570 {E} and {G,F} and for block 2015 {E} and {E, S, A}.

Let estimate resanable count of possible affected addresses for SPV wallet within batch. On the assumption 2-3 payments daily we have about 42 affected addresses in a 2 weeks or 2016 blocks range. Siutable parameters for GCS is <code>M = 45 P = 5</code>, this means that duplicated elements coded with small number of bits. As you can see on picture, 4 bits per duplicate element and 20-22 bits per unique element.




==== Commitment ====

This is one of the useful properties that filters should meet, filter construction should use deterministic algorithm to give ability publish commitment for filter like witness commitment is in an OP_RETURN output of the coinbase transaction.
By adding miners to the task, creating filters, checking content and making commitment, unlock the protection for spv nodes from concealing payments at the consensus level. Filters commitment is optional and could be added infuture soft fork.



==== Total filters size ====
Total size for filters 0 -> 596,735 blocks.

===== BIP 158 =====

Total filters size is about 4.3 Gb


===== Batch 2016 blocks =====

* total size: 3.32 Gb
** <code>P2PK+P2PKH</code> addresses: 2.56 Gb (1.97 + 0.58)
** <code>P2SH</code> addresses: 0.67 Gb (0.51 + 0.16)
** <code>P2WPKH</code> addresses: 87 Mb  (72 + 15)
** <code>P2WSH</code> addresses: 7 Mb (4 + 3)

~ 8.88 Mb buffer size required to process filters

22%  less space than  BIP 158

===== Batch 4032 blocks =====

* total size: 3.14 Gb
** <code>P2PK+P2PKH</code> addresses: 2.41 Gb (1.80 + 0.61)
** <code>P2SH</code> addresses: 0.63 Gb (0.47 + 0.17)
** <code>P2WPKH</code> addresses: 85 Mb (69 + 15)
** <code>P2WSH</code> addresses: 7 Mb (4 + 3)

~ 16.25  Mb buffer size required to process filters

27%  less space than  BIP 158

===== Batch 8064 blocks =====

* total size: 3.0 Gb
** <code>P2PK+P2PKH</code> addresses: 2.3 Gb (1.67 + 0.63)
** <code>P2SH</code> addresses: 0.61 Gb (0.43 + 0.17)
** <code>P2WPKH</code> addresses: 84 Mb (67 + 16)
** <code>P2WSH</code> addresses: 7 Mb (4 + 3)

~ 30.1  Mb buffer size required to process filters

30%  less space than  BIP 158

== Specification ==

For each block, filters are derived containing sets of addresses associated with the block. Filter is probabilistic structure which matches all items in the set with probability 1, and matches other items with probability 1/M for some integer parameter M. <code>M = Z * K * L</code>, <code>Z</code> - number of monitoring addresses, <code>K</code> - number of blocks in batch, <code>L</code> - number of batches. In other words one false positive result within L batches with K blocks each, for set of Z addresses. This BIP use <code>M = 10,000 * 4,032 * 10</code>   To construct this probabilistic structure, a set of unique hashed elements is used, where the maximum bit length of the element determines the frequency of false positives. To reduce size of filter used Golomb-Rice coding the same schema like in BIP 158 and called Golomb-coded set - GCS.

At a high level, a GCS is constructed from a set of <code>N</code> items by:
# hashing all items to 64-bit integers in the range <code>[0,  N * M)</code>
# sorting the hashed values in ascending order
# computing the differences between each value and the previous one
# writing the differences sequentially, compressed with Golomb-Rice coding


The hashing shema is same as BIP 158 except SipHash function parameter.

==== Hashing Data Objects ====

Siphash function used in BIP 158 is effective function computes 64-bit or 128-bit values from a variable-length message and 128-bit secret key. For a 128-bit secret key, the first 128-bit of block hash is used. Consequently the same element but in different blocks has a different hashes. As a result of this we have to recalculate all hashes for monitoring elements for each block what adds unnecessary computing overhead. 

SipHash is fundamentally different from cryptographic hash functions like SHA and main goal is a produce message authentication code: a keyed hash function like HMAC. To create a probabilistic filter structure, an effective hash function with good uniform distribution is needed. MAC functionality is out of block filters task scope. Using a constant value of 128-bit key allows using sighash as a regular hash function and opens up the opportunity for filters batching.

128-bit key <code>k</code> should be set to 0 for all filters.

Siphash parameters: <code>k = 0</code> <code>c = 2</code> and <code>d = 4</code>.



== Block Batch Filters ==

This BIP defines list of  new filter types:

Unique block addresses in batch range:

* <code>P2PK+P2PKH - 0x01</code>
* <code>P2SH  - 0x02</code>
* <code>P2WPKH  - 0x04</code>
* <code>P2WPSH  - 0x08</code>

Duplicated block addresses in batch range:

* <code>P2PK+P2PKH - 0x10</code>
* <code>P2SH  - 0x20</code>
* <code>P2WPKH  - 0x40</code>
* <code>P2WPSH  - 0x80</code>

Filter types support filter combinations, that means in case we need block filters for all address types we should calculate
0x01 + 0x02 + 0x04 + 0x08 + 0x10 + 0x20 + 0x40 + 0x80 = 0xff. To reduce bandwith usage light client should request 
only unique addresses filters fisrt and only for affected block batches download duplicates.

==== Filters contents ====

The filters contain set of unique items  derived from transaction inputs and outputs hashes for standard script types (pubkey hashes, script hashes). Coinbase inputs are not used. This information is minimum sufficient for light wallet to sync blockchain. Script/Pubkey hashes is choosen instead of raw script because output script can’t be recovered for P2PK addresses in case no information about public key. This means that watch only wallet, with imported P2PK addresses from base58 format, not able to reconstruct output script without downloading transactions where this script was seen first time and will not be able to check filters.

Each transaction has a set of UNIQUE address elements involved in it. The block filter consists of 2 parts. First part is hashed set with unuque elements within filter batch range. The second part is a set of duplicated elements plus a sequence with duplicate counters.

Block filter created as follows: 

* first transaction set with UNIQUE elements compare with all block filters in batch include recently created block empty filter 
* all unique items within the batch added to block filter set with unique elemets
* all duplicates are added to the set with duplicate elements and a counter is incremented for each element of the duplicate
* apply first 3 steps for each block transaction sequentially

It is important to delete duplicates within transaction sets and count duplicates within block set. This aproach give ability 
to compare downloaded block filter  with set of transactions filters independently received one by one from mempool. Block filters verified by commitment and duplicates counter make it possible to unequivocally conclude that all transactions was received with correct filters. In case counters is not equal it means that the nodes that sent the transaction filters tried to hide payments from the light client. More details here https://github.com/bitaps-com/bips/blob/master/bip-mempool-transactions-filters.mediawiki.


==== Filter structure ====

{|
! Field Name !! Type !! Size  || Purpose
|-
| cumulative_address_count || CompactSize || 1 - 5 bytes  || Value exist only in first block filter in range of batch representing sum of all unique elements in previous batches. Genesis block filter has initial value 10,000
|-
| unique_items_count || CompactSize || 1 - 5 bytes || Count of filter unique elements
|-
| number_unique_set_bytes || Binary blob || variable || A variable length integer representing the size of the GCS with uniqie elements
|-
| unique_set || Binary blob || variable || GCS with unique  elements
|-
| duplicates_items_count || CompactSize || 1 - 5 bytes || Count of filter duplicate elements
|-
| number_duplicates_set_bytes || CompactSize || 1 - 5 bytes || A variable length integer representing the size of the GCS with  duplicate elements
|-
| duplicates_set || Binary blob || variable || GCS with  duplicate elements
|-
| number_duplicates_counters_bytes || CompactSize || 1 - 5 bytes || A variable length integer representing the length of the duplicates_counter sequnce.
|-
| duplicates_counters ||  Binary blob  || variable || Sequence of counters of duplicates
|-
|}

The first filters batch started from Genesis block and have initial value for <code>cumulative_address_count = 10,000</code>. Each subsequent block adds the number of unique elements in the previous to <code>cumulative_address_count</code>. GCS parameters for batch calculated from <code>cumulative_address_count</code>.

<code>P = floor(log2( cumulative_address_count / 1.497137)) </code>

<code>M = ceil(1.49713 * 2^P) </code>

This parameters is close to optimal<ref>https://gist.github.com/sipa/576d5f09c3b86c3b1b75598d799fc845</ref>.

==== Duplicates counters ====

Huffman coded sequence of duplicates counters. This sequence representing count of corresponding duplicated element. 

==== Commitment structure ====

A commitment is an important part of block filters for future consensus layer update, that provides protection against payment hiding for light clients. Without commitment light clients rely on the fact that most of the nodes in the network do not hide payments. Tha same asumption was in BIP37. To combat concealment of payments, light client must receive information from several independent nodes, but it still vulnerable for sybil attack. Adding block filters commitment to coinbase transaction, as the block validation rule will improve the lightweight client protocol by moving from trust to validation on consensus layer. 

<img src=bip-block-batch-filters/commitment.png></img>

Block filters commitment hash is merkle root of merkle tree structure, where tree leafs is GCS filter hashes linked with previous block GCS filter hashes. Linking to previous block hashes unlock ability to download batch of filter headers and one commitment from last block to verify all. In case we download commitments for each block, we get a lot of overhead because get a block commitment means that we need to load the coinbase transaction and all block transaction hashes to make sure that the coinbase transaction matches the merkle root.

== Light client filter processing schema ==

TODO

== Compatibility ==

Older clients remain fully compatible and interoperable after this change. 

Clients which do not implement this BIP remain fully compatible after this change using existing protocols, because this is 
additional node service and changes not affected to exist protocol.

== Reference Implementation ==
TODO

== References ==

<references/>

== Copyright ==

This document is placed in the public domain.
