<pre>
  BIP: Block Batch Filters for Light Clients
  Layer: Peer Services
  Title: Block Batch Filters for Light Clients
  Author: Aleksey Karpov <admin@bitaps.com>
  Comments-Summary: None yet
  Comments-URI: 
  Status: Draft
  Type: Standards Track
  Created: 2019-09-20
  License: CC0-1.0
</pre>
== Work in progress ==

==== To do ====
* if use filter elements as addreess or script hashes test diffirence if we exclude siphash and just get first 64 bit of already calculated hashes
* determine the optimal range for batch 

  

== Abstract ==

This BIP describes a new filter types on blocks data, for use in the BIP 157 light client protocol. 
The filter construction proposed is an alternative to BIP 158 filters, with lower filters size, lower false positive rate and  separated by address types.

== Motivation ==

Block filters designed to help light clients analize block contents without downloading full blocks and minimize the expected bandwidth consumed. The main difference from BIP 37 approach is no privacy leakage. BIP 158 defines initial filter type <code>basic</code>. This BIP proposed more effective and flexible alternative

Fisrt off all lets determine reasanable number of addresses tracked by a light client in most common case.

To achieve privacy and security, wallets must follow the rule - no address reuse. This means that each payment
received or sent uses one or more new addresses. The wallet makes 2-3 payments per day, during the year the number 
of used addresses increases to about 1000 pieces. All addresses that were used at least once should be monitored
for new payments. Assume in this BIP that a reasonable estimated number of wallet monitoring addresses is 10 000 pieces. 

Сonsider the key parameters that the filter should match:


==== False positive rate ====

False positive rate should not be high and provide effective filtering and reduce bandwidth consumprion. False positive rate in BIP 158 is not low enough. Lower bandwidth consumption can be achieved with a lower false positive filter and a larger fiters size with the same scheme. Below provided result of test with  10,000 addresses in monitoring for 2,000 blocks (7,300 addresses each block):

<pre>
Parameters for GCS: M=784931 P=19
False positive blocks 32 from 2,000 
Filters size:  36.64 Mb
Downloaded 68.64 Mb (32 blocks included);
</pre>

Size of downloaded false positive blocks (~32 Mb) can be excluded by use filters with lower false positive rate . 

<pre>
Parameters for GCS: M=54975581 P=25
False positive blocks 0 from 2,000
Download 57.64 Mb  
Filters size:  57.64 Mb
</pre>

Bandwidth consumption reduced about 16%. 

==== Total filters size and adaptive FPS rate ====

Total filters size for BIP 158 is about 4.3 Gb ( 596,200 blocks). To achieve a smaller filters size, should take a more flexible approach. Use of optimal and flexible parameters for false positive rate the key of success.

We get this opportunity using a flexible approach to false positive rate parameters in accordance with the count of addresses in  blockchain. Currently about 560 millions unique addresses stored in bitcoin blockchain and this value grows with each block. To get expected false positive rate we should use <code>M=560,000,000</code>  as parameter used in BIP 158. Unique addresses count in blockchain:
* 0 - 100,000 blocks  174,718
* 0 - 200,000 blocks  6,577,160
* 0 - 300,000 blocks  35,535,921
* 0 - 400,000 blocks  128,802,764
* 0 - 500,000 blocks  346,316,499

To reduce filters size we should use adaptive and optimal parameters <code>C</code> - cumulitive count of unique blocks addresses until given block,  <code>P = floor(log2(C / 1.497137))</code>, <code> M= 1.49713 * 2^P </code>.

==== Commitment ====

This is one of the useful properties that filters should meet, filter construction should use deterministic algorithm to give ability publish commitment for filter like witness commitment is in an OP_RETURN output of the coinbase transaction.
By adding miners to the task, creating filters, checking content and making commitment, unlock the protection for spv nodes from concealing payments at the consensus level. Filters commitment is optional and could be added infuture soft fork.

==== Batching ====

About 94% of all addresses recorded in bitcoin blockchain have zero balance at this moment, this means that at 
least 2 records for 94% of addresses in blockchain. Each spent utxo have 1 creating and 1 spending records. The presence of duplicate elements is also due to a lot of addresses was reused.

Combining filters for ranges of blocks gives another opportunity to reduce the size of filters. Divide the filter elements into 2 parts. First one is uniqe elements within range of <code>Q</code> blocks and second part is duplicated element in same block range. 

On picture below you can see first part of unique addresses for block filters highlighted in blue. Duplicated elements below, have a distinctive feature. They form a separate set for encoding which a sufficiently high FPS is enough, that means save alot of space.

<img src=bip-block-batch-filters/batching_b.png></img>

For example, an SPV wallet keeps track set of addresses {Q,V,C,E}. On block 1 we have positive test and download this block. On block 2 we have no positive test on first part and then we check second part. The second part consists only of duplicates from the beginning of the block till recent block. Our task is get check is this duplicated elements belong to our addresses {Q,V,C,E}. To solve this task we should compare set of  duplicates from recent block with a set of addresses we track. But we can reduce our monitoring set to a smaller set, consisting of the tracked addresses that were used from the beginning of the range till recent block. For block 2 on example compare {E} and {B}, for block 570 {E} and {G,F} and for block 2015 {E} and {E, S, A}.

Let estimate resanable count of possible affected addresses for SPV wallet within batch. On the assumption 2-3 payments daily we have about 42 affected addresses in a 2 weeks or 2016 blocks range. Siutable parameters for GCS is <code>M = 45 P = 5</code>, this means that duplicated elements coded with small number of bits. As you can see on picture, 4 bits per duplicate element and 20-22 bits per unique element.

Comparsion BIP 158 and different block ranges bacthes :

  Blocks 144 * 7 = 1008  [450,000 -> 451,008)
  BIP158 size  = 17 Mb
  1008 blocks batch size = 14.23 Mb (unique - 11.87 Mb; duplicates - 2.32 Mb)
  GCS parameteres:  M = 401882775; P = 28;
  space saving - 16.2 %

  Blocks 144 * 14 = 2016  [450,000 -> 452,016)
  BIP158 size  = 34 M
  2016 blocks batch size = 28.25 Mb (unique - 22.35 Mb; duplicates - 5.83 Mb)
  GCS parameteres:  M = 401882775; P = 28;
  space saving - 16.9 %

  Blocks 144 * 21 = 3024  [450,000 -> 453,024)
  BIP158 size  = 51 M
  3024 blocks batch size = 41.23 Mb (unique - 31.38 Mb; duplicates - 9.76 Mb)
  GCS parameteres:  M = 401882775; P = 28;
  space saving - 19.1 %



---



BIP 158 use siphash as hash function to produce set of filter elements. Siphash is effective function computes 64-bit 
or 128-bit values from a variable-length message and 128-bit secret key. For a 128-bit secret key, the first 128-bit of 
block hash is used. Consequently the same element but in different blocks has a different hashes. As a result of this
we have to recalculate all hashes for monitoring elements for each block and we can’t aggregate block filters sets  to batch. 

SipHash is fundamentally different from cryptographic hash functions like SHA and main goal is a produce message 
authentication code: a keyed hash function like HMAC. To create a probabilistic filter structure, an effective hash 
function with good uniform distribution is needed. MAC functionality is out of block filters task scope. Using a constant
value of 128-bit key allows using sighash as a regular hash function and opens up the opportunity for filters batching.

The 64-bit SipHash outputs are then mapped uniformly over the desired range by multiplying with F and taking the top 
64 bits of the 128-bit result. Where F = N * M, N - count of filter elements, M - inverse of  probability for set of 
elements matches other items.  This algorithm is a faster alternative to modulo reduction, as it avoids the expensive 
division operation. In other words, 64-bit SipHash outputs are mapped to values with a given number of bits 
to achieve a given false positive rate. To archive shotter filter size with Golomb-Rice coding N should be 
exactly match the count of elements in filter. This means that filters with fewer elements cut out more bits from hashes 
than filters with more elements. As a result, we cannot do batching for filters due to the lack of reduced bits and 
still have to recalculate all hashes for wallet monitoring elements for each block.

In case we use static parameters for Golomb-Rice coding F, with necessary bits and false positive rate for all blocks 
independently from block filter elements count. Redundancy size grows significantly for the filters with the count 
of elements moving away from the maximum count.

In this BIP describes alternative filter coding algorithm based on combination of Deltas coding and double Huffman coding, 
which allows to minimize the size redundancy for filters with a small number of elements and approaches the compression up
to 98,3 % from optimal,  for filters with an average and a large number of elements.


== Specification ==

For each block, filters are derived containing sets of addresses or scripts associated with the block. 

At a high level, a batch filter is constructed from a set of <code>N</code> items by:
* hashing all items to 64-bit integers in the range <code>[0, F)</code>
* sorting the hashed values in ascending order and remove duplicates
* apply Deltas coding - computing the differences between each value and the previous one
* writing the differences sequentially to bit string with minimal bits required to encode each value
* writing separate bit string with number of bits for corresponding value in first bitstring
* compress second bit string with 2 round Huffman coding 

==== Hashing data objects ====

The first step in the filter construction is hashing the variable-sized raw items in the set to the range <code>[0, F)</code>, where <code> F = N * M</code>.  Currently about 560 millions unique addresses stored in bitcoin blockchain, reasonable values of <code> N, M </code>  defined in this BIP is <code> N = 10 000 </code> (count of unique addresses monitored by wallet), <code> M = 1 000 000 000 </code> (count of unique addresses in blockchain). This parameters may be increased in the future with the growth of the blockchain.

The items are first passed through the pseudorandom function SipHash, which takes a 128-bit key <code> k = 0 </code> and a variable-sized byte vector and produces a uniformly random 64-bit output. Implementations of this BIP MUST use the SipHash parameters <code>c = 2</code> and <code>d = 4</code> for best performance.

The 64-bit SipHash outputs are then mapped uniformly over the desired range by multiplying with <code>F</code> and taking the top 64 bits of the 128-bit result. This algorithm is a faster alternative to modulo reduction, as it avoids the expensive division operation, the same as in BIP 158. 

==== Deltas coding ====

Instead of writing the items in the hashed set directly to the filter, greater compression is achieved by only writing the differences between successive items in sorted order.

==== Huffman coding ====

Instead of encoding values into a sequence of bits with a prefix that allows decoding, we divide the data into 2 separate sequences.  First sequence with  element values written  as a string of bits using the minimum required number bits for each element. Second sequence with number of bits written in first sequence for each element, written as byte string. As result we got first sequence without redundancy and second one with a lot of duplicate values. 

Huffman encoding is effective algorithm to encode duplicate values in sequence. 2 round of huffman encoding on second sequence give optimal result. To reduce size of compressed second sequence we will use minimal  bits threshold for writing values for first sequence. Count of bits to encode most of item values is in range 20 - 24, we apply 20 bit minimal threshold to reduce huffman coding dictionary, get more duplicates in second sequence and get better huffman compression result.

<pre>
def encode_dhcs(elements, min_bits_threshold=20):
   # Delta-Hoffman coded set
   data_sequence = bitarray()
   data_sequence_append = data_sequence.append

   deltas_bits = deque()
   deltas_bits_map_freq = dict()
   last = 0

   for value in  sorted(elements):
       delta =  value - last

       bits = delta.bit_length()
       if bits < min_bits_threshold:
           bits =  min_bits_threshold

       deltas_bits.append(bits)

       try:
           deltas_bits_map_freq[bits] += 1
       except:
           deltas_bits_map_freq[bits] = 1

       while bits > 0:
           data_sequence_append(delta & (1 << (bits - 1)))
           bits -= 1
       last = value

   # huffman encode round 1
   # encode bits length sequence to byte string
   codes_round_1 = huffman_code(huffman_tree(deltas_bits_map_freq))
   r = bitarray()
   r.encode(codes_round_1, deltas_bits)
   bits_sequence = r.tobytes()
   bits_sequnce_len_round_1 = r.length()

   # huffman encode round 2
   # encode byte string
   deltas_bits = deque()
   deltas_bits_map_freq = dict()
   for i in bits_sequence:
       b = i >> 4
       c = i & 0b1111
       deltas_bits.append(b)
       try:
           deltas_bits_map_freq[b] += 1
       except:
           deltas_bits_map_freq[b] = 1

       deltas_bits.append(c)
       try:
           deltas_bits_map_freq[c] += 1
       except:
           deltas_bits_map_freq[c] = 1

   codes_round_2 = huffman_code(huffman_tree(deltas_bits_map_freq))
   r = bitarray()
   r.encode(codes_round_2, deltas_bits)
   bits_sequnce_len_round_2 = r.length()
   bits_sequence = r.tobytes()


   code_table_1 = int_to_var_int(len(codes_round_1))
   for code in codes_round_1:
       code_table_1 += int_to_var_int(code)
       code_table_1 += int_to_var_int(codes_round_1[code].length())
       code_table_1 += b"".join([bytes([i]) for i in codes_round_1[code].tolist()])

   code_table_2 = int_to_var_int(len(codes_round_2))
   for code in codes_round_2:
       code_table_2 += int_to_var_int(code)
       code_table_2 += int_to_var_int(codes_round_2[code].length())
       code_table_2 += b"".join([bytes([i]) for i in codes_round_2[code].tolist()])


   d_filter_len = data_sequence.length()
   d_filter_string = data_sequence.tobytes()

   return  b"".join((code_table_1,
                     code_table_2,
                     int_to_var_int(bits_sequnce_len_round_1),
                     int_to_var_int(bits_sequnce_len_round_2),
                     bits_sequence,
                     int_to_var_int(d_filter_len),
                     d_filter_string))

</pre>

<pre>
def decode_dhcs(h):
   # Delta-Hoffman coded set
   stream = get_stream(h)

   # read code_table_1
   c = var_int_to_int(read_var_int(stream))
   code_table_1 = dict()
   for i in range(c):
       key = var_int_to_int(read_var_int(stream))
       l = var_int_to_int(read_var_int(stream))
       code =  bitarray([bool(k) for k in stream.read(l)])
       code_table_1[key] = code

   # read code_table_2
   c = var_int_to_int(read_var_int(stream))
   code_table_2 = dict()
   for i in range(c):
       key = var_int_to_int(read_var_int(stream))
       l = var_int_to_int(read_var_int(stream))
       code =  bitarray([bool(k) for k in stream.read(l)])
       code_table_2[key] = code

   # read compressed deltas
   deltas_bits_len_1 = var_int_to_int(read_var_int(stream))
   deltas_bits_len_2 = var_int_to_int(read_var_int(stream))
   deltas_byte_len = deltas_bits_len_2 // 8 + int(bool(deltas_bits_len_2 % 8))

   r = stream.read(deltas_byte_len)
   deltas = bitarray()
   deltas.frombytes(r)

   while deltas.length() > deltas_bits_len_2:
       deltas.pop()

   # Huffman decode round 1
   r = deltas.decode(code_table_2)
   deltas_string = bytearray()
   for i in range(int(len(r)/2)):
       deltas_string += bytes([(r[i*2] << 4) + r[i*2 + 1]])

   # Huffman decode round 2
   r = bitarray()
   r.frombytes(bytes(deltas_string))

   while r.length() > deltas_bits_len_1:
       r.pop()

   deltas_bits = r.decode(code_table_1)


   d_filter_bit_len = var_int_to_int(read_var_int(stream))
   d_filter_byte_len = d_filter_bit_len // 8 + int(bool(d_filter_bit_len % 8))
   r = stream.read(d_filter_byte_len)

   d_filter = bitarray()
   d_filter.frombytes(r)

   while d_filter.length() > d_filter_bit_len:
       d_filter.pop()

   f = 0
   f_max = d_filter.length()
   decoded_set = set()
   last = 0

   for bits in deltas_bits:
       d = 0
       while bits  > 0 and f < f_max :
           bits -= 1
           d = d << 1
           if d_filter[f]:
               d += 1
           f += 1
       last += d
       decoded_set.add(last)
      
   return decoded_set

</pre>

== Block Batch Filters ==

This BIP defines list of  filter types:

* block 
  P2PK+P2PKH  - 0x01, P2SH  - 0x02, P2WPKH  - 0x03, P2WPSH  - 0x04
* batch level 1
  P2PK+P2PKH  - 0x05, P2SH  - 0x06, P2WPKH  - 0x07, P2WPSH  - 0x08
  size - 2 500 000 unique addresses + addresses till end of last block in ran
* batch level 2
  P2PK+P2PKH  - 0x09, P2SH  - 0x0A, P2WPKH  - 0x0B, P2WPSH  - 0x0C 
  size - 10 000 000 unique addresses + addresses till end of last block in range



==== Filters contents ====

The block batch filters contain set off unique items  derived from hashes of standard script types (pubkey hashes, script hashes). This information is minimum sufficient for light wallet to sync blockchain. Output script can’t be recovered for P2PK addresses in case no information about public key. This means that watch only wallet, with imported P2PK addresses from base58 format, not able to reconstruct output script without downloading transactions where this script was seen first time. For this reason hashes are used instead of output scripts.

Batching set of filter items for blocks effectively reduces filter size by removing duplicates. Separation of filters by address type, allows lite client not to download redundant information without compromising privacy. It will significantly reduce global bandwidth usage, because the most of wallets are processing one type of addresses.

To save space on full node and reduce traffic, sequential creation of batch filters block by block should be done by recording difference of elements.

<img src=bip-block-batch-filters/batches1.png></img>

On this image shown how duplicate elements E with 22 bits, stored in batches as difference from batch with higher level.

Lite client sync schema (BIP 157):

* send message getcfheaders for batch level 2 filters 
* in case avalable filters
** send message getcfilters and download all filters for level 2 batches
** in case positive test
*** send getcfheaders for batch level 2 and range of blocks within affected level 2 fiter
*** send message getcfilters and download filters for level 1 batches
*** detect affected blocks range
*** send message getcfilters and download block filters
*** detect affected blocks
*** send message getdata and download affected blocks
* send message getcfheaders for batch level 1 filters for rest blocks range not included in last level 2 filter
* in case avalable filters
** send message getcfilters and download all filters for level 1 batches
** in case positive test 
*** send message getcfilters and download block filters
*** detect affected blocks
*** send message getdata and download affected blocks
* send message getcfilters and download block filters for blocks not included to last level 1 filter
* in case positive test 
** send message getdata and download affected blocks

<img src=bip-block-batch-filters/batches2.png></img>

Once higher level filter created, lower filters MUST move up all elements except duplicates.

== Compatibility ==

This block batch filter construction is not incompatible with existing software, though it requires implementation of the new filters.

== Reference Implementation ==

Delta-Huffman coding: https://github.com/bitaps-com/pybtc/blob/bugfix/pybtc/functions/filters.py#L172

== Copyright ==

This document is licensed under the Creative Commons CC0 1.0 Universal license.
