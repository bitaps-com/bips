<pre>
  BIP: Block Batch Filters for Light Clients
  Layer: Peer Services
  Title: Block Batch Filters for Light Clients
  Author: Aleksey Karpov <admin@bitaps.com>
  Comments-Summary: None yet
  Comments-URI: 
  Status: Draft
  Type: Standards Track
  Created: 2019-09-20
  License: PD
</pre>

== Abstract ==

This BIP describes a new filter types on blocks data, for use in the BIP 157 light client protocol. 
The filter construction proposed is an alternative to BIP 158 filters, with lower filters size, lower false positive rate and  separated by address types.

== Motivation ==

Block filters are designed to help light client analysis of block contents without loading full blocks and minimizing expected bandwidth consumption. The main difference from the BIP 37 approach is no privacy leakage and no Denial of Service (DoS) attack vector for nodes. BIP 158 defines initial filter type <code>basic</code>. This BIP proposed more effective and flexible alternative.

First of all let's determine reasonable number of addresses tracked by a light client in most common case. We will use this value for false positive rate tests in this BIP.

To achieve privacy and security, wallets must follow the "no address reuse" rule. This means that each payment
received or sent, uses one or more new addresses. Suppose wallet makes 2-3 payments daily, during the year the number 
of used addresses increases to about 1000 pieces. All addresses that were used at least once should be monitored
for new payments. Assume in this BIP that a reasonable estimated number of wallet monitoring addresses is 10,000, which corresponds to 10 years of active use of the wallet.

We also assume that the proper wallet implementation that safeguards user privacy,  when requesting transaction, blocks or other data from the Bitcoin network uses the TOR network, and generate new ip and new identity for each request.


The main block filter parameters that have been improved and redesigned in this BIP:


==== Separating filters by address type ====

Separating filters by address type, allows light client not to download redundant information without compromising privacy. It will significantly reduce global bandwidth usage, because the most of wallets are processing only one type of addresses.


==== Transaction filters ====

More details about transaction filter described here https://github.com/bitaps-com/bips/blob/master/bip-mempool-transactions-filters.mediawiki.

The idea of transaction filters is to make it possible to take advantage of filtering for unconfirmed transactions, as well as reduce bandwidth consumption when request only those transactions that are necessary for an light client when we have a positive test in a block filter.



==== Commitment ====

This is one of the useful properties that filters should meet, filter construction should use deterministic algorithm to give ability create and publish commitment for filters like witness commitment is in an OP_RETURN output of the coinbase transaction. Adding the task of creating filters to the rules of consensus, forcing miners to create filters and make filters commitment for each block. This will unlocks the protection of spv-nodes against payment-hiding attacks. At the current moment, light clients  trust the nodes and rely on the fact that most nodes are honest.

Adding block filters commitment to coinbase transaction, as the block validation rule will improve the light client protocol by moving from trust to validation on consensus layer. Filters described here are fully adapted for this improvement but the filters commitment is beyond the scope of this BIP and may be added in a future soft fork.


==== Total filters size and false positive rate ====

The frequency of false positives should not be high and provide effective filtering to reduce bandwidth consumption, but to achieve this we must increase the information entropy of the elements in filters, which leads to an increase in size. The most effective algorithm for data compression and the balance between the size of the filters and the number of false positives are the main tasks to be solved in this BIP.

Total filters size calcalted for range  0 -> 596,735 blocks.

* <b>BIP 158</b> total size: 4.3 Gb
* <b>Batch filters</b> total size: 3.36 Gb (batch size 1008 blocks)

Batch filters size about 22% smaller than the BIP 158.


==== Batching ====

About 94% of all addresses recorded in bitcoin blockchain have zero balance at this moment, which means that at least 2 entries for 94% of addresses written in the block chain. Each spent utxo have 1 creating and 1 spending record. The presence of duplicate elements is also due to the fact that many addresses are used several times.

Combining filters into batches of blocks makes it possible to reduce the size of filters. Dividing the filter into 2 parts allows encoding data more compactly. The first part is the unique addresses for the block in the batch. The second part is duplicate elements in the current block, which have already met in the first part of filters in the batch. But duplicates for unique elements of the current block are not recorded, since the entry test will be successfully passed in the first part of the block filter. 

On the picture below shows the structure of the batch. Unique addresses for block filters highlighted in blue. All unique elements are indexed. Duplicates are written as pointers instead of the values themselves. 

<img src=bip-block-batch-filters/batchf.png></img>

On picture <code>Block 2</code> have address <code>E</code> as duplicate element written as pointer 5. <code>Block 570</code> have addresses <code>G</code> and <code>E</code>  as duplicate element, written as pointers 6 and 5. In <code>Block 1567</code> shown duplicated element <code>J</code>, but this duplicated element is redundant and should be omitted because our first part of filter already have positive test for element <code>J</code>. 

Separation of unique elements and duplicates also gives some improvement in the filter scanning speed, because checking the second part of the filter is only necessary if we have affected addresses in recent batch and we need check only small set of affected addresses.

Returning to the example above, if we are tracking a set of addresses <code>{R, J, Q}</code> only starting after <code>Block 570</code>, we need to check the second part with duplicates. Starting from <code>Block 571</code>, we need to check only the one element <code>Q</code>, and then from <code>Block 572</code> we need to check set <code>{R, Q}</code> for each block duplicates. In other words, it’s enough for us to check for duplicates part only for set of affected monitoring elements in recent batch, in case false positive test element related to monitoring address also considered as affected.

Using a batch approach with pointers to unique elements, we got a good degree of data compression. On average to encode 1 unique element (4 bytes in size) we need only 2.59 bytes and about 1.29 bytes to encode 1 duplicated element, which together gives 1.98 bytes per block address element (compression rate 50.5%).


==== Light client filters test perfomance ====

BIP 158 filters use an array with the size of the elements <code>N * M</code> to map all possible block elements hashes. Where <code>N</code> count of elements in block and <code>M = 784931</code> constant represent <code>1/M</code> probabilty rate. In this case we are dealing with floating parameter <code>F = N * M</code> for Golomb coding, also BIP 158 use randomization vector for siphash function for each block. In other words, to check the block filter, we need to recalculate for each block a new array of addresses that are monitored by light wallet in accordance with the randomization vectors and the parameter <code>F</code>.
In case we want rescan all blocks and check 10 000 addresses we have to calculate about 6 000 000 000 hash + map_into_range functions.

In this BIP purposed shema with constat parameter <code>F = 2 ^ 32</code> and dinamicaly calculated parameter <code>P</code> for Golomb coding. No randomization vectors for each block and constant <code>F</code> remove computation overhead with almost same false positive rate (false positive test result slightly better than BIP 158). Also an additional speed improvement is related to fact that the design of divided filters into 2 parts allows us to check part 2 only for set of addresses that have a positive test in the first part of filters within current batch. This reduces the number of elements to be checked by almost half.


== Specification ==

==== Filter types ====

* <code>P2PK - 0x01</code>
* <code>P2PKH  - 0x02</code>
* <code>P2SH  - 0x04</code>
* <code>P2WPKH  - 0x08</code>
* <code>P2WPSH  - 0x16</code>

Filter types support filter combinations, which means that in case we need block filters for all address types we should calculate 0x01 + 0x02 + 0x04 + 0x08 + 0x16 = 0x25. To reduce bandwidth usage, the light client should request only the necessary filter types. In case combined filter types are requested, the filters are serialized sequentially according to the type index.

==== Filter elements ====

The filters elements derived from transaction inputs and outputs hashes for standard script types (pubkey hashes, script hashes). Coinbase inputs are not used. This information is minimum sufficient for light wallet to sync blockchain. Script/Pubkey hashes is choosen instead of raw script because output script can’t be recovered for P2PK addresses in case no information about public key. This means that watch only wallet, with imported P2PK addresses from base58 format, not able to reconstruct output script without downloading transactions where this script was seen first time and will not be able to check filters.

Filter is probabilistic structure which matches all items in the set with probability 1, and matches other items with probability 1/M for some integer parameter M. In BIP 158 all elements hashed with sipHash and map into set of distinct integers within the range <code>[0, F)</code>, where <code>F = N * M</code>, <code>N</code> is count of block elements and M is BIP 158 constant <code>M = 784931</code>.

In this BIP used sipHash without randomization vectors. Implementations of this BIP MUST use the SipHash parameters c = 2 and d = 4 and 128-bit key k = 0. For mapping set of sipHash result integers within the range <code>[0, F)</code> used constant parameter <code>F= 2 ^ 32</code>. False positve probability is <code>N / F</code>. For compression, the Golomb-Rice code with the parameter P is used, where P is determined for each filter based on the values in the set. This approach allows us to preserve the simplicity and speed of sets comparison process, exclude computation overhead for each set.


==== Transaction filter ====

For each transaction, filters are derived containing sets of unique elements from standard scripts associated with transaction. Set of unique elements are hashed with sipHash function and mapped into array with the size <code>F = 2 ^ 32</code>. After calculations, the elements are serialized into a byte string. More details descibed here https://github.com/bitaps-com/bips/blob/master/bip-mempool-transactions-filters.mediawiki.


==== Block filter ====

For each block, filters are derived containing sets of elements associated with the block transaction filters. 
All block standard scripts are divided by type of script and a separate filter is formed for each type. The filter consists of unique elements, duplicates in the form of pointers and a fingerprint of transaction filters. A transaction filter fingerprint is a hash from serialized block transaction filters. Fingerprint is used for the following purposes.
In case we have positive test result in block filter we can get transaction information by 2 methods. The first method is to request a complete block from the bitcoin network, the second is more effective - request filters for all transactions of the block and determine the exact transaction that we must request from the network. Download transaction filters is about 40-60 kbyte of data instead of 1-1.2 MB complete block. 

This BIP is fully adopted for block filters commitment and transaction filters fingerprint give for light client ability to verify transaction filters with commitment. The following example shows the need to use the fingerprint for transaction filters.

Light client downloaded block filter and got positive test result for monitoring address. Positive test means that one or more block transactions are affected. Block filter could be verified via commitment (we assume that a filters commitment already accepted as block validation rule), but when the light client requests transaction filters, we cannot be sure that the received filters are correct, and the sender node is not trying to hide payments. 

Light client can use block merkle root to determine that all transaction filters are received. But we can't verify content of this filters. Malicious node could craft transaction filters and show only 1 transaction instead of 2 or more. Because we have no counters of elements in block filters we can't determain this attack. Transaction filters fingerprint solve this problem by serializing transaction filters using a deterministic algorithm and obtain hash fingerprint from this data. In case fingerprint not match light client should request complete block and ban malicious node from connection pool.

Block filter created as follows: 

# serialize all block transaction filters and get fingerprint
# create elements set with block unique elements within recent batch
# create duplicates set with pointers for all duplicated elements within recent batch
# encode elements set with Golomb coding
# encode duplicates set with Golomb coding
# serialize filter and get commitment hash

It is important to note that transaction filters consist only of unique elements, where uniqueness is determined within the transaction.



==== Filter structure ====

{|
! Field Name !! Type !! Size  || Purpose
|-
| elements_set_bytes || CompactSize || 1 - 5 bytes  || A variable length integer representing the size of the Golomb Coded Set with uniqie elements
|-
| elements || Binary blob || variable || GCS with unique  elements
|-
| duplicates_set_bytes || CompactSize || 1 - 5 bytes || A variable length integer representing the size of the GCS with  duplicate elements
|-
| duplicates || Binary blob || variable || GCS with  duplicates 
|-
| fingerprint ||  Binary blob  || 20 bytes || block transaction filters fingerprint (in case filter items not exists, this field is omitted)
|-
|}

==== Filter header ====

Filter header defined as <code>header = SHA256(previous_header +  SHA256(serialized_block_filter))</code>. In case previous header not exist used 32 zero bytes as value. Chaining filter headers is another one adoption step for filter block  commitments softfork.  This allows to request coinbase transaction and merkle proof for this transaction only for the last block. Otherwise, we would have to download coinbase transaction for each block to be able verify block filters with commitment. Chained headers provide cryptographic proof of the validity of all previous filters in case last one is verified.


==== Commitment structure ====

Block filters commitment this is merkle root from merkle tree constructed from filter headres. At example merkle tree until segwit softfork constructed from filter types: 0x01, 0x02, 0x04. After segwit softfork and first transaction with new standard script types we should construct merkle tree from next filer types:  0x01, 0x02, 0x04, 0x08, 0x16.  Merkle tree construction algorith is same as block transaction merkletree (double sha256 hash function, odd tree levels are normalized by duplicating the last hash)


== Light client filter processing schema ==

TODO

== Compatibility ==

Older clients remain fully compatible and interoperable after this change. 

Clients which do not implement this BIP remain fully compatible after this change using existing protocols, because this is 
additional node service and changes not affected to exist protocol.

== Reference Implementation ==
TODO

== References ==

<references/>

== Copyright ==

This document is placed in the public domain.
