<pre>
  BIP: Block Batch Filters for Light Clients
  Layer: Peer Services
  Title: Block Batch Filters for Light Clients
  Author: Aleksey Karpov <admin@bitaps.com>
  Comments-Summary: None yet
  Comments-URI: 
  Status: Draft
  Type: Standards Track
  Created: 2019-09-20
  License: CC0-1.0
</pre>

== Work in progress and open questions ==

==== Optimal range for batch?  ====

Determine optimal range for batch

  Blocks 144 * 14 = 2016  [450,000 -> 452,016)
  BIP158 size  = 34 M
  2016 blocks batch size = 28.25 Mb (unique - 22.35 Mb; duplicates - 5.83 Mb)
  GCS parameteres:  M = 401882775; P = 28;
  space saving - 16.9 %


  Blocks 144 * 28 = 4032  [450,000 -> 454,032)
  BIP158 size  = 69 M
  4032 blocks batch size = 54.85 Mb (unique - 40.67 Mb; duplicates - 14.05 Mb)
  GCS parameteres:  M = 401882775; P = 28;
  space saving - 20.5 %



==== Why we need siphash?  ====
Why we need siphash and not just use first 64 bits from addreess or script hashes ?

todo: test diffirence if we exclude siphash and just get first 64 bit of already calculated hashes


==== Tests ====
Todo:finish testing and determine exactly size for each filter type for mainnet.


------------------------------------
  

== Abstract ==

This BIP describes a new filter types on blocks data, for use in the BIP 157 light client protocol. 
The filter construction proposed is an alternative to BIP 158 filters, with lower filters size, lower false positive rate and  separated by address types.

== Motivation ==

Block filters designed to help light clients analize block contents without downloading full blocks and minimize the expected bandwidth consumed. The main difference from BIP 37 approach is no privacy leakage. BIP 158 defines initial filter type <code>basic</code>. This BIP proposed more effective and flexible alternative

Fisrt off all lets determine reasanable number of addresses tracked by a light client in most common case.

To achieve privacy and security, wallets must follow the rule - no address reuse. This means that each payment
received or sent uses one or more new addresses. The wallet makes 2-3 payments per day, during the year the number 
of used addresses increases to about 1000 pieces. All addresses that were used at least once should be monitored
for new payments. Assume in this BIP that a reasonable estimated number of wallet monitoring addresses is 10 000 pieces. 

Сonsider the key parameters that the filter should match:


==== False positive rate ====

False positive rate should not be high and provide effective filtering and reduce bandwidth consumprion. False positive rate in BIP 158 is not low enough. Lower bandwidth consumption can be achieved with a lower false positive filter and a larger fiters size with the same scheme. Below provided result of test with  10,000 addresses in monitoring for 2,000 blocks (7,300 addresses each block):

<pre>
Parameters for GCS: M=784931 P=19
False positive blocks 32 from 2,000 
Filters size:  36.64 Mb
Downloaded 68.64 Mb (32 blocks included);
</pre>

Size of downloaded false positive blocks (~32 Mb) can be excluded by use filters with lower false positive rate . 

<pre>
Parameters for GCS: M=54975581 P=25
False positive blocks 0 from 2,000
Download 57.64 Mb  
Filters size:  57.64 Mb
</pre>

Bandwidth consumption reduced about 16%. 

==== Total filters size and adaptive FPS rate ====

Total filters size for BIP 158 is about 4.3 Gb ( 596,200 blocks). To achieve a smaller filters size, should take a more flexible approach. Use of optimal and flexible parameters for false positive rate the key of success.

We get this opportunity using a flexible approach to false positive rate parameters in accordance with the count of addresses in  blockchain. Currently about 560 millions unique addresses stored in bitcoin blockchain and this value grows with each block. To get expected false positive rate we should use <code>M=560,000,000</code>  as parameter used in BIP 158. Unique addresses count in blockchain:
* 0 - 100,000 blocks  174,718
* 0 - 200,000 blocks  6,577,160
* 0 - 300,000 blocks  35,535,921
* 0 - 400,000 blocks  128,802,764
* 0 - 500,000 blocks  346,316,499

To reduce filters size we should use adaptive and optimal parameters <code>C</code> - cumulitive count of unique blocks addresses until given block,  <code>P = floor(log2(C / 1.497137))</code>, <code> M= 1.49713 * 2^P </code>.

==== Commitment ====

This is one of the useful properties that filters should meet, filter construction should use deterministic algorithm to give ability publish commitment for filter like witness commitment is in an OP_RETURN output of the coinbase transaction.
By adding miners to the task, creating filters, checking content and making commitment, unlock the protection for spv nodes from concealing payments at the consensus level. Filters commitment is optional and could be added infuture soft fork.

==== Batching ====

About 94% of all addresses recorded in bitcoin blockchain have zero balance at this moment, this means that at 
least 2 records for 94% of addresses in blockchain. Each spent utxo have 1 creating and 1 spending records. The presence of duplicate elements is also due to a lot of addresses was reused.

Combining filters for ranges of blocks gives another opportunity to reduce the size of filters. Divide the filter elements into 2 parts. First one is uniqe elements within range of <code>Q</code> blocks and second part is duplicated element in same block range. 

On picture below you can see first part of unique addresses for block filters highlighted in blue. Duplicated elements below, have a distinctive feature. They form a separate set for encoding which a sufficiently high FPS is enough, that means save alot of space.

<img src=bip-block-batch-filters/batching_b.png></img>

For example, an SPV wallet keeps track set of addresses {Q,V,C,E}. On block 1 we have positive test and download this block. On block 2 we have no positive test on first part and then we check second part. The second part consists only of duplicates from the beginning of the block till recent block. Our task is get check is this duplicated elements belong to our addresses {Q,V,C,E}. To solve this task we should compare set of  duplicates from recent block with a set of addresses we track. But we can reduce our monitoring set to a smaller set, consisting of the tracked addresses that were used from the beginning of the range till recent block. For block 2 on example compare {E} and {B}, for block 570 {E} and {G,F} and for block 2015 {E} and {E, S, A}.

Let estimate resanable count of possible affected addresses for SPV wallet within batch. On the assumption 2-3 payments daily we have about 42 affected addresses in a 2 weeks or 2016 blocks range. Siutable parameters for GCS is <code>M = 45 P = 5</code>, this means that duplicated elements coded with small number of bits. As you can see on picture, 4 bits per duplicate element and 20-22 bits per unique element.

Comparsion BIP 158 and different block ranges bacthes :

  Blocks 144 * 7 = 1008  [450,000 -> 451,008)
  BIP158 size  = 17 Mb
  1008 blocks batch size = 14.23 Mb (unique - 11.87 Mb; duplicates - 2.32 Mb)
  GCS parameteres:  M = 401882775; P = 28;
  space saving - 16.2 %

  Blocks 144 * 14 = 2016  [450,000 -> 452,016)
  BIP158 size  = 34 M
  2016 blocks batch size = 28.25 Mb (unique - 22.35 Mb; duplicates - 5.83 Mb)
  GCS parameteres:  M = 401882775; P = 28;
  space saving - 16.9 %

  Blocks 144 * 21 = 3024  [450,000 -> 453,024)
  BIP158 size  = 51 M
  3024 blocks batch size = 41.23 Mb (unique - 31.38 Mb; duplicates - 9.76 Mb)
  GCS parameteres:  M = 401882775; P = 28;
  space saving - 19.1 %


  Blocks 144 * 28 = 4032  [450,000 -> 454,032)
  BIP158 size  = 69 M
  4032 blocks batch size = 54.85 Mb (unique - 40.67 Mb; duplicates - 14.05 Mb)
  GCS parameteres:  M = 401882775; P = 28;
  space saving - 20.5 %


==== Separation of filters by address type ====

Separation of filters by address type, allows light client not to download redundant information without compromising privacy. It will significantly reduce global bandwidth usage, because the most of wallets are processing one type of addresses.


== Specification ==

For each block, filters are derived containing sets of addresses associated with the block. Filter is probabilistic structure which matches all items in the set with probability 1, and matches other items with probability 1/M for some integer parameter M. To construct this probabilistic structure used set of uniqe hashed elements where maximum bit length on of element defines false positive rate. To reduce size of filter used Golomb-Rice coding the same schema like in BIP 158 and called Golomb-coded set - GCS.

At a high level, a GCS is constructed from a set of <code>N</code> items by:
# hashing all items to 64-bit integers in the range <code>[0, N * M)</code>
# sorting the hashed values in ascending order
# computing the differences between each value and the previous one
# writing the differences sequentially, compressed with Golomb-Rice coding


The hashing shema is same as BIP 158 except SipHash function parameter.

==== Hashing Data Objects ====

Siphash function used in BIP 158 is effective function computes 64-bit or 128-bit values from a variable-length message and 128-bit secret key. For a 128-bit secret key, the first 128-bit of block hash is used. Consequently the same element but in different blocks has a different hashes. As a result of this we have to recalculate all hashes for monitoring elements for each block what adds unnecessary computing overhead. 

SipHash is fundamentally different from cryptographic hash functions like SHA and main goal is a produce message authentication code: a keyed hash function like HMAC. To create a probabilistic filter structure, an effective hash function with good uniform distribution is needed. MAC functionality is out of block filters task scope. Using a constant value of 128-bit key allows using sighash as a regular hash function and opens up the opportunity for filters batching.

128-bit key <code>k</code> should be set to 0 for all filters.

Siphash parameters: <code>k = 0</code> <code>c = 2</code> and <code>d = 4</code>.


== Block Batch Filters ==

This BIP defines list of  new filter types:

Full filters:

* P2PK+P2PKH - 0x01
* P2SH  - 0x02
* P2WPKH  - 0x03
* P2WPSH  - 0x04

Only unique addresses part:

* P2PK+P2PKH - 0x05
* P2SH  - 0x06
* P2WPKH  - 0x07
* P2WPSH  - 0x08

Only duplicated addresses part:

* P2PK+P2PKH - 0x09
* P2SH  - 0x0A
* P2WPKH  - 0x0B
* P2WPSH  - 0x0C

==== Filters contents ====

The block batch filters contain set off unique items  derived from hashes of standard script types (pubkey hashes, script hashes). This information is minimum sufficient for light wallet to sync blockchain. Output script can’t be recovered for P2PK addresses in case no information about public key. This means that watch only wallet, with imported P2PK addresses from base58 format, not able to reconstruct output script without downloading transactions where this script was seen first time. For this reason hashes are used instead of output scripts.

==== Filter structure ====

{|
! Field Name !! Type !! Size  || Purpose
|-
| cumulitive_address_count || CompactSize || 1 - 5 bytes  || Value exist only in first block filter in range of batch representing sum of all unique elements in previous batches. Genesis block filter has initial value 10,000
|-
| unique_items_count || CompactSize || 1 - 5 bytes || Count of filter unique elements
|-
| number_unique_set_bytes || Binary blob || variable || A variable length integer representing the size of the GCS with uniqie elements
|-
| unique_set || Binary blob || variable || GCS with unique  elements
|-
| duplicate_items_count || CompactSize || 1 - 5 bytes || Count of filter duplicate elements
|-
| number_duplicate_set_bytes || CompactSize || 1 - 5 bytes || A variable length integer representing the size of the GCS with  duplicate elements
|-
| duplicate_set || Binary blob || variable || GCS with  duplicate elements
|}

The first filters batch started from Genesis block and have initial value for <code>cumulitive_address_count = 10,000</code>. Each subsequent block adds the number of unique elements in the previous to <code>cumulitive_address_count</code>. GCS parameters for batch calculated from <code>cumulitive_address_count</code>.

<code>P = floor(log2( cumulitive_address_count / 1.497137)) </code>

<code>M = ceil(1.49713 * 2^P) </code>

==== Commitment structure ====

A commitment is an important part of block filters for future consensus layer update, that provides protection against payment hiding for light clients. Without commitment light clients rely on the fact that most of the nodes in the network do not hide payments. Tha same asumption was in BIP37. Adding block filters commitment to coinbase transaction, as the block validation rule will improve the lightweight client protocol by moving from trust to validation. 

<img src=bip-block-batch-filters/commitment.png></img>

Block filters commitment hash is merkle root or merkle tree structure, where tree leafs if hashes from GCS filters parts linked with previous commitment hash. Linking to previous commitment hash unlock ability to download filters headers for 

== Light client filter processing schema ==

work in progress ...

== Compatibility ==

This block batch filter construction is not incompatible with existing software, though it requires implementation of the new filters.

== Reference Implementation ==
...

== Copyright ==

This document is licensed under the Creative Commons CC0 1.0 Universal license.
