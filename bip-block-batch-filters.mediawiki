<pre>
  BIP: Block Batch Filters for Light Clients
  Layer: Peer Services
  Title: Block Batch Filters for Light Clients
  Author: Aleksey Karpov <admin@bitaps.com>
  Comments-Summary: None yet
  Comments-URI: 
  Status: Draft
  Type: Standards Track
  Created: 2019-09-20
  License: PD
</pre>

== Abstract ==

This BIP describes a new filter types on blocks data, for use in the BIP 157 light client protocol. 
The filter construction proposed is an alternative to BIP 158 filters, with lower filters size, lower false positive rate and  separated by address types.

== Motivation ==

Block filters designed to help light clients analize block contents without downloading full blocks and minimize the expected bandwidth consumed. The main difference from the BIP 37 approach is the аbsence of privacy leakage and the Denial of Service (DoS) attack vector for nodes. BIP 158 defines initial filter type <code>basic</code>. This BIP proposed more effective and flexible alternative.

Fisrt of all lets determine reasanable number of addresses tracked by a light client in most common case.

To achieve privacy and security, wallets must follow the "no address reuse" rule. This means that each payment
received or sent uses one or more new addresses. Suppose wallet makes 2-3 payments per day, during the year the number 
of used addresses increases to about 1000 pieces. All addresses that were used at least once should be monitored
for new payments. Assume in this BIP that a reasonable estimated number of wallet monitoring addresses is 10 000 pieces. 

Key parameters that the new block filters correspond:

==== Total filters size and false positive rate ====

The frequency of false positives should not be high and provide effective filtering to reduce bandwidth consumption, but to achieve this we must increase the information entropy of the elements, which leads to an increase in size. The most effective algorithm for data compression and the balance between the size of the filters and the number of false positives are the main tasks to be solved in this BIP.

Total filters size calcalted for range  0 -> 596,735 blocks.

* <b>BIP 158</b> total size: 4.3 Gb
* <b>Batch 1008 blocks</b> total size: 3.4 Gb

Batch filters size about 20% smaller than the BIP 158.


==== Separating filters by address type ====

Separating filters by address type, allows light client not to download redundant information without compromising privacy. It will significantly reduce global bandwidth usage, because the most of wallets are processing only one type of addresses.


==== Commitment ====

This is one of the useful properties that filters should meet, filter construction should use deterministic algorithm to give ability publish commitment for filters like witness commitment is in an OP_RETURN output of the coinbase transaction. Adding the task of creating filters to the rules of consensus, forcing miners to create filters and make filters commitment, unlocks the protection of spv-nodes against payment-hiding attacks. At the current moment, lightweight clients  trust the nodes and rely on the fact that most nodes are honest.

Adding block filters commitment to coinbase transaction, as the block validation rule will improve the lightweight client protocol by moving from trust to validation on consensus layer. Filters described here are fully adapted for this improvement but the filters commitment is beyond the scope of this BIP and may be added in a future soft fork.

==== Transaction filters ====

Using transaction filters makes it possible to use the advantages of filtering for unconfirmed transactions, as well as requesting only those transactions that are necessary for the light client when we have positive test in block filters.
More details decribed here https://github.com/bitaps-com/bips/blob/master/bip-mempool-transactions-filters.mediawiki.



==== Batching ====

About 94% of all addresses recorded in bitcoin blockchain have zero balance at this moment, this means that at 
least 2 records for 94% of addresses in blockchain. Each spent utxo have 1 creating and 1 spending records. The presence of duplicate elements is also due to the fact that many addresses are used several times.

Combining filters into batches of blocks makes it possible to reduce the size of filters. Dividing the filter into 2 parts allows encoding data more compactly. The first part is the unique addresses for the block in the batch. The second part is duplicate elements in the current block, which have already met in the first part of filters in the batch. But duplicates for unique elements of the current block are not recorded, since the entry test will be successfully passed in the first part of the block filter. 

On the picture below shows the structure of the batch. Unique addresses for block filters highlighted in blue. All unique elements are indexed. Duplicates are written as pointers instead of the values themselves. 

<img src=bip-block-batch-filters/batchf.png></img>
On picture <code>Block 2</code> have address <code>E</code> as duplicate element. <code>Block 570</code> have address <code>G</code>. Separation of unique elements and duplicates also gives some improvement in the filter scanning speed, because checking the second part of the filter is only necessary if we have affected addresses in recent batch and we need check only small set of affected addresses.
Returning to the example above, if we are tracking a set of addresses <code>{A, J, Q}</code> only starting from <code>Block 570</code> (inclusive), we need to check the part with duplicates. Starting with <code>Block 570</code>, we need to check only the element <code>J</code>, and then from <code>Block 1567</code> we need to check set <code>{J, Q}</code>, in other words, it’s enough for us to check for duplicates only for our affected elements in the last batch.

Using a batch approach with pointers to unique elements, described in detail below, we get a good degree of data compression. To encode 1 unique element (4 bytes in size) we need only 2.6 bytes and about 1.4 bytes to encode 1 duplicated element with a counter.







== Specification ==

For each block, filters are derived containing sets of addresses associated with the block. Filter is probabilistic structure which matches all items in the set with probability 1, and matches other items with probability 1/M for some integer parameter M. <code>M = Z * K * L</code>, <code>Z</code> - number of monitoring addresses, <code>K</code> - number of blocks in batch, <code>L</code> - number of batches. In other words one false positive result within L batches with K blocks each, for set of Z addresses. This BIP use <code>M = 10,000 * 4,032 * 10</code>   To construct this probabilistic structure, a set of unique hashed elements is used, where the maximum bit length of the element determines the frequency of false positives. To reduce size of filter used Golomb-Rice coding the same schema like in BIP 158 and called Golomb-coded set - GCS.

At a high level, a GCS is constructed from a set of <code>N</code> items by:
# hashing all items to 64-bit integers in the range <code>[0,  N * M)</code>
# sorting the hashed values in ascending order
# computing the differences between each value and the previous one
# writing the differences sequentially, compressed with Golomb-Rice coding


The hashing shema is same as BIP 158 except SipHash function parameter.

==== Hashing Data Objects ====

Siphash function used in BIP 158 is effective function computes 64-bit or 128-bit values from a variable-length message and 128-bit secret key. For a 128-bit secret key, the first 128-bit of block hash is used. Consequently the same element but in different blocks has a different hashes. As a result of this we have to recalculate all hashes for monitoring elements for each block what adds unnecessary computing overhead. 

SipHash is fundamentally different from cryptographic hash functions like SHA and main goal is a produce message authentication code: a keyed hash function like HMAC. To create a probabilistic filter structure, an effective hash function with good uniform distribution is needed. MAC functionality is out of block filters task scope. Using a constant value of 128-bit key allows using sighash as a regular hash function and opens up the opportunity for filters batching.

128-bit key <code>k</code> should be set to 0 for all filters.

Siphash parameters: <code>k = 0</code> <code>c = 2</code> and <code>d = 4</code>.



== Block Batch Filters ==

This BIP defines list of  new filter types:

Unique block addresses in batch range:

* <code>P2PK+P2PKH - 0x01</code>
* <code>P2SH  - 0x02</code>
* <code>P2WPKH  - 0x04</code>
* <code>P2WPSH  - 0x08</code>

Duplicated block addresses in batch range:

* <code>P2PK+P2PKH - 0x10</code>
* <code>P2SH  - 0x20</code>
* <code>P2WPKH  - 0x40</code>
* <code>P2WPSH  - 0x80</code>

Filter types support filter combinations, that means in case we need block filters for all address types we should calculate
0x01 + 0x02 + 0x04 + 0x08 + 0x10 + 0x20 + 0x40 + 0x80 = 0xff. To reduce bandwith usage light client should request 
only unique addresses filters fisrt and only for affected block batches download duplicates.

==== Filters contents ====

The filters contain set of unique items  derived from transaction inputs and outputs hashes for standard script types (pubkey hashes, script hashes). Coinbase inputs are not used. This information is minimum sufficient for light wallet to sync blockchain. Script/Pubkey hashes is choosen instead of raw script because output script can’t be recovered for P2PK addresses in case no information about public key. This means that watch only wallet, with imported P2PK addresses from base58 format, not able to reconstruct output script without downloading transactions where this script was seen first time and will not be able to check filters.

Each transaction has a set of UNIQUE address elements involved in it. The block filter consists of 2 parts. First part is hashed set with unuque elements within filter batch range. The second part is a set of duplicated elements plus a sequence with duplicate counters.

Block filter created as follows: 

* first transaction set with UNIQUE elements compare with all block filters in batch include recently created block empty filter 
* all unique items within the batch added to block filter set with unique elemets
* all duplicates are added to the set with duplicate elements and a counter is incremented for each element of the duplicate
* apply first 3 steps for each block transaction sequentially

It is important to delete duplicates within transaction sets and count duplicates within block set. This aproach give ability 
to compare downloaded block filter  with set of transactions filters independently received one by one from mempool. Block filters verified by commitment and duplicates counter make it possible to unequivocally conclude that all transactions was received with correct filters. In case counters is not equal it means that the nodes that sent the transaction filters tried to hide payments from the light client. More details here https://github.com/bitaps-com/bips/blob/master/bip-mempool-transactions-filters.mediawiki.


==== Filter structure ====

{|
! Field Name !! Type !! Size  || Purpose
|-
| cumulative_address_count || CompactSize || 1 - 5 bytes  || Value exist only in first block filter in range of batch representing sum of all unique elements in previous batches. Genesis block filter has initial value 10,000
|-
| unique_items_count || CompactSize || 1 - 5 bytes || Count of filter unique elements
|-
| number_unique_set_bytes || Binary blob || variable || A variable length integer representing the size of the GCS with uniqie elements
|-
| unique_set || Binary blob || variable || GCS with unique  elements
|-
| duplicates_items_count || CompactSize || 1 - 5 bytes || Count of filter duplicate elements
|-
| number_duplicates_set_bytes || CompactSize || 1 - 5 bytes || A variable length integer representing the size of the GCS with  duplicate elements
|-
| duplicates_set || Binary blob || variable || GCS with  duplicate elements
|-
| number_duplicates_counters_bytes || CompactSize || 1 - 5 bytes || A variable length integer representing the length of the duplicates_counter sequnce.
|-
| duplicates_counters ||  Binary blob  || variable || Sequence of counters of duplicates
|-
|}

The first filters batch started from Genesis block and have initial value for <code>cumulative_address_count = 10,000</code>. Each subsequent block adds the number of unique elements in the previous to <code>cumulative_address_count</code>. GCS parameters for batch calculated from <code>cumulative_address_count</code>.

<code>P = floor(log2( cumulative_address_count / 1.497137)) </code>

<code>M = ceil(1.49713 * 2^P) </code>

This parameters is close to optimal<ref>https://gist.github.com/sipa/576d5f09c3b86c3b1b75598d799fc845</ref>.

==== Duplicates counters ====

Huffman coded sequence of duplicates counters. This sequence representing count of corresponding duplicated element. 

==== Commitment structure ====

A commitment is an important part of block filters for future consensus layer update, that provides protection against payment hiding for light clients. Without commitment light clients rely on the fact that most of the nodes in the network do not hide payments. Tha same asumption was in BIP37. To combat concealment of payments, light client must receive information from several independent nodes, but it still vulnerable for sybil attack. Adding block filters commitment to coinbase transaction, as the block validation rule will improve the lightweight client protocol by moving from trust to validation on consensus layer. 

<img src=bip-block-batch-filters/commitment.png></img>

Block filters commitment hash is merkle root of merkle tree structure, where tree leafs is GCS filter hashes linked with previous block GCS filter hashes. Linking to previous block hashes unlock ability to download batch of filter headers and one commitment from last block to verify all. In case we download commitments for each block, we get a lot of overhead because get a block commitment means that we need to load the coinbase transaction and all block transaction hashes to make sure that the coinbase transaction matches the merkle root.

== Light client filter processing schema ==

TODO

== Compatibility ==

Older clients remain fully compatible and interoperable after this change. 

Clients which do not implement this BIP remain fully compatible after this change using existing protocols, because this is 
additional node service and changes not affected to exist protocol.

== Reference Implementation ==
TODO

== References ==

<references/>

== Copyright ==

This document is placed in the public domain.
